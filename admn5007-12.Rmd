---
title:
author: 
date: 
output: revealjs::revealjs_presentation
css: docs/styles.css
incremental: true
slideNumber: true
highlight: pygments
knit: (function(input_file, encoding) {
  out_dir <- 'docs';
  rmarkdown::render(input_file,
 encoding=encoding,
 output_file=file.path(dirname(input_file), out_dir, 'index.html'))})
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## A/B Testing: Experimental Design  {data-background="art.jpg" .title}
March 26, 2019

## What is A/B Testing?
A/B testing is a framework for you to test different ideas for how to improve upon an existing design. 

<img src="docs/ab-testing-funnel.png" alt="Funnel of three A/B tests including ads, landing pages and welcome email" style="float:left; display: inline; margin-right: 30px;">

<small>Image Source: [Optimizely: Optimization Glossary](https://www.optimizely.com/optimization-glossary/ab-testing/)</small>

For marketing, this often means testing:

> - Promotional offers
> - Copy
> - Form fields
> - Page layout
> - Conversion rates
> - Drop off rates
> - Time spent on a website
> - Email subject lines

## A/B Testing Process

![Image of A/B Testing Process](docs/ab-testing-process-6.png)

<small>Image Source: [Optimizely: Optimization Glossary](https://www.optimizely.com/optimization-glossary/ab-testing/)</small>

## Example
<small>All examples are fictious.</small>

Lets say that we are interested in knowing if <b>Build your business</b> performs better than <b>Create your business</b> in terms of how long users stay on the homepage.

![Control Version of Shopify website with purple Start Your Free Trial button](docs/control.jpg)

## Variables

<img src="docs/control.jpg" alt="Control Version of Shopify website with purple Start Your Free Trial button" style="float:left; display: inline; margin-right: 30px;">

> - <b>Question</b>: How long are users staying on our homepage?
> - <b>Hypothesis</b>: Users stay on the site longer when they see <b>Build your business</b>. 
> - <b>Dependent Variable</b>: Time on site.
> - <b>Independent Variable</b>: The condition of Build versus Create.

## Common Statistical Tests for A/B Testing

<b>T-test</b>: Allows us to compare two groups with a continuous dependent variable.

An example for when you would use a t-test to evaluate an A/B test would be to measure, for instance, time (how long are individuals staying on my website) as the dependent variable.

<b>Logistic Regression</b>: Allows us to compare two groups with binary (categorical) dependent variable. 

An example for when you would use logistic regression to evaluate an A/B test, might be for conversions (did the user complete the action or not).

![Control and variation graph](docs/control-variation-graph.png)

<small>Image Source: [Optimizely: Optimization Glossary](https://www.optimizely.com/optimization-glossary/ab-testing/)</small>

## Running a t-test
To get the dataset, visit Blackboard and download the 'time.csv' file.

```{r condition, message=FALSE, warning=FALSE}
create_build_eval <- read.csv("time.csv")
head(create_build_eval)
```
## Running a t-test

```{r message=FALSE, warning=FALSE}
experiment_results <- t.test(time_spent_homepage_sec ~ condition, data = create_build_eval)
experiment_results
```

## Example - Test Version
<small>All examples are fictious.</small>

![Test Version of Shopify website with green Start Your Free Trial button](docs/test.jpg)

## Variables
<img src="docs/ab-testing.png" alt="Control version of a website compared to a test version of a website" style="float:left; display: inline; margin-right: 30px;">
<small>Image Source: [Optimizely: Optimization Glossary](https://www.optimizely.com/optimization-glossary/ab-testing/)</small>


> - <b>Question</b>: Will making a colour change to my button result in more button clicks?
> - <b>Hypothesis</b>: By changing the colour to 'green' it will result in more button clicks. 
> - <b>Dependent Variable</b>: Whether or not the user clicked the button.
> - <b>Independent Variable</b>: Button colour.


## Evaluate Preliminary Dataset

```{r clicks, message=FALSE, warning=FALSE}
library(tidyverse)

clicks <- read.csv("click.csv")
clicks
```

## Calculate Conversion Rate
Calculate the conversion rate by using the 'dplyr' function <b>summarize</b> to find the means of the clicks.
```{r}
library(dplyr)
clicks <- read.csv("click.csv")

clicks %>%
  summarize(conversion_rate = mean(clicked_button))
```

## Factors
We computed the rate over the year, but there could be other factors that affect the number of clicks. We'll compute the conversion rate of clicks by month to see if there is a change month to month. 

```{r message=FALSE, warning=FALSE}
library(lubridate)

clicks %>%
  group_by(month(visit_date)) %>%
  summarize(conversion_rate = mean(clicked_button))
```

## Plot the Results
```{r message=FALSE, warning=FALSE}
library(ggplot2)

clicks_sum <- clicks %>%
  group_by(month(visit_date)) %>%
  summarize(conversion_rate = mean(clicked_button))

clicks_sum_plot <- ggplot(clicks_sum, aes(x = `month(visit_date)`, y = conversion_rate)) + 
  geom_point() + 
  geom_line()
```

## View Plot

```{r echo=FALSE, fig.height=5}
plot(clicks_sum_plot)
```
We are able to see that there is significant difference month-to-month in terms of the number of conversions.


## Load Experiment File

```{r message=FALSE, warning=FALSE}
library(tidyverse)

experiment_data <- read_csv("experiment.csv")
experiment_data
```

## Experiment Results

```{r message=FALSE, warning=FALSE}

experiment_data_sum <- experiment_data %>%
  group_by(visit_date, condition) %>% 
  summarize(conversion_rate = mean(clicked_button))

experiment_data_sum
```

## Plot Experiment Results

```{r message=FALSE, warning=FALSE}
experiment_data_plot <- ggplot(experiment_data_sum, 
       aes(x = visit_date, y = conversion_rate, 
           color = condition, 
           group = condition)) +
  geom_point() +
  geom_line()
```

## View Plot 
```{r echo=FALSE}
plot(experiment_data_plot)
```

## Analyze our Experiment Results
```{r message=FALSE, warning=FALSE}
library(tidyverse)
library(broom)

experiment_data <- read_csv("experiment.csv")

glm(clicked_button ~ condition, 
    family = "binomial", 
    data = experiment_data) %>%
  tidy()
```

## Let's Try It!
In this example, we want to know if by moving our <b>Like</b> button on a page, if it will result in more clicks on the button.

<img src="docs/control2.jpg" alt="LinkedIn Article original mockup" style="float:left; max-width: 45%;">

<img src="docs/test2.png" alt="LinkedIn Article with Like button moved up above the fold" style="float:right; max-width: 45%;">

## Answer
```{r like, message=FALSE, warning=FALSE}
library(tidyverse)
## Load File
like_data <- read_csv("like.csv")

## Evaluate Conversions
like_data_sum <- like_data %>%
  group_by(visit_date, condition) %>% 
  summarize(conversion_rate = mean(clicked_like))

## Plot Data
like_plot <- ggplot(like_data_sum, 
       aes(x = visit_date, y = conversion_rate, 
           color = condition, 
           group = condition)) +
  geom_point() +
  geom_line()

## Analyze Experiment
library(tidyverse)
library(broom)

glm(clicked_like ~ condition, 
    family = "binomial", 
    data = like_data) %>%
  tidy()
```

## Power Analysis
A/B testing tries to run both conditions simultaneously as opposed to separately so that both are exposed to the same variables. 

A power analysis will tell you how many data points (or sample size) you’ll need to ensure the effect is real.

There are lots of sample size calculators online. For example, [SurveyMonkey](https://www.surveymonkey.com/mp/sample-size-calculator/) has an online calculator that lets you calculate sample size.

> - <b>Statistical Test</b>: Test you plan to run
> - <b>Baseline Value</b>: Value for the current condition
> - <b>Desired Value</b>: Expected value for the test condition
> - <b>B</b>: Proportion of the data (ideally 0.5)
> - <b>Alpha</b>: Level where effect is significant (generally 0.05)
> - <b>Power</b>: Probability of rejecting the null hypothesis (generally 0.8)

## Running a Power Analysis

```{r message=FALSE, warning=FALSE}
library(powerMediation)

total_sample_size <- SSizeLogisticBin(p1 = 0.2,
                                      p2 = 0.3,
                                      B = 0.5,
                                      alpha = 0.05,
                                      power= 0.8)

total_sample_size
```

## What do I do if my results are not significant?
<b>Stopping rules</b> procedure that allows for interim analysis at predefined times. Because of te predefined stop-times, this has to be built into your experiment plan.

> Stopping rules are also known as <b>Sequential Analysis</b>

Once you hit the predefined stop-times, there are three choices you can make. You can:

> - Stop the experiment because you have statistical signicance
> - Stop the experiment because it didn't work
> - Continue the experiment because not enough data has been collected

## Sequential Analysis

```{r message=FALSE, warning=FALSE}
library(gsDesign)

seq_analysis <- gsDesign(k = 4, 
                         test.type = 1, 
                         alpha = 0.05, 
                         beta = 0.2, 
                         sfu = "Pocock")

seq_analysis
```

## Sample Size for Sequential Analysis

<b>Resource-based</b>: figure out the maximum number of data points we are willing to collect.

```{r message=FALSE, warning=FALSE}
library(gsDesign)

seq_analysis <- gsDesign(k = 4, 
                         test.type = 1, 
                         alpha = 0.05, 
                         beta = 0.2, 
                         sfu = "Pocock")

max_n <- 1000
max_n_per_group <- max_n / 2

stopping_points <- max_n_per_group * seq_analysis$timing

stopping_points
```

## A/B Testing Vs. Multivariate Testing

Sometimes, the goal of an experiment is to see whether or not two different independent variables affect one another. 

![Example of Multivariate Testing with four different options](docs/multivariate.jpg)

Rather than running two separate A/B tests, we could analyze the performance through multivariate testing (which tests to see if there is an interaction effect between the variables).

## Assignment 5: Creating our own Experiment {.title}
<b>Due date: Tuesday, April 9, 2019</b>

For this assignment, you will be working in pairs. You can choose your partner. 

In your team, you will form a hypothesis about an email campaign for one of your favorite new or existing products. For example, a hypothesis might be: “An email campaign for a company that sells outdoor gear will convert (i.e., cause a customer to finalize a purchase) more with a subject line with a promotional message plus call to action (e.g., ‘50% Off All Jackets. Buy Now!’) than one with a promotional message alone (e.g., ‘50% Off All Jackets.’).

Using [Mailchimp](https://mailchimp.com/), you will determine what you would like to test (i.e. subject line.) and create your email.

## Assignment 5: Setting up your Campaign
<b>Due date: Tuesday, April 9, 2019</b>

A couple of items to set up your campaign:

- Your population size is 50 - you must find at least 50 people to receive your email.
- At least 39 people will have had to open the email of your original population.
- In order to be CASL compliant, you must provide an opt-in mechanism to your subscribers (i.e. you can’t subscribe people to your email list - they must elect to join). MailChimp has this function available. 

<b>Helpful hint: Only register for ONE MailChimp Account per team.</b>

You will be required to submit <b>one report</b> on the performance of your hypothesis. The outline for what needs to be in your report is on Blackboard. Please confirm who your partner is before the end of the class. 

Also include all of your data and your work to determine whose results were successful. 

Helpful reading for running an A/B test with MailChimp: [https://mailchimp.com/help/create-an-ab-testing-campaign/](https://mailchimp.com/help/create-an-ab-testing-campaign/)




